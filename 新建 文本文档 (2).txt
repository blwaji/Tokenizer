1.修改切分，结合模型
2.检索，生成，短文本对长文本，复杂语境

相关问题，相关答案
垂直式问答，相关问题，问答对

语义，向量检索，数据清洗，大模型百川


1.切分时，加一个用模型切分的参数，传入模型，文件名，输出list（Document）
2.添加问答对检索，问答对和文档的向量存入不同文件
2.1纯问答对检索，FAISS
2.2问答对和文档共同检索

9月7日
切分新闻文本，对总分结构和非总分结构做不同处理
总分结构：根据段尾的冒号和下文开头的数字进行判断
非总分结构：按字数划分

10月25日
1、微调baichuan2-13b-chat，初始学习率为5e-6，轮次为15，数据量1500
2、使用微调的模型生成答案
3、调研大模型加速推理，Medusa：增加几个解码头从而加快速度
github地址：https://github.com/FasterDecoding/Medusa



llama2

CUDA_VISIBLE_DEVICES=0 python src/train_bash.py --stage sft --model_name_or_path path_to_llama_model --do_train --dataset self_cognition  --template llama2 --finetuning_type lora --lora_target q_proj,v_proj --output_dir saves/LLaMA-13B/lora/2023-10-17-14-00-08 --overwrite_cache --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_steps 50 --learning_rate 5e-5 --num_train_epochs 3.0 --plot_loss --fp16

baichuan

CUDA_VISIBLE_DEVICES=0 python src/train_bash.py --stage sft --model_name_or_path /home/wangguangle/model/baichuan-13b-Chat --do_train --dataset self_cognition  --template baichuan --finetuning_type lora --lora_target W_pack --output_dir saves/Baichuan-13B-Chat/lora/2023-10-23-17-50 --overwrite_cache --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_steps 10 --learning_rate 1e-5 --num_train_epochs 15.0 --plot_loss --fp16


baichuan2

CUDA_VISIBLE_DEVICES=0 python src/train_bash.py --stage sft --model_name_or_path /home/wangguangle/model/baichuan2-13b-chat --do_train --dataset self_cognition  --template baichuan2 --finetuning_type lora --lora_target W_pack --output_dir saves/Baichuan2-13B-Chat/lora/2023-10-24-14-51 --overwrite_cache --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --lr_scheduler_type cosine --logging_steps 10 --save_steps 10 --learning_rate 1e-5 --num_train_epochs 15.0 --plot_loss --fp16


Export model

python src/export_model.py --model_name_or_path /home/wangguangle/model/baichuan-13b-Chat --template baichuan --finetuning_type lora --checkpoint_dir /home/wangguangle/LLaMA-Factory-0.2.0/saves/Baichuan-13B-Chat/lora/2023-10-23-13-49/checkpoint-440 --export_dir /home/wangguangle/model/baichuan-13b-Chat-2023-10-23-13-49-440 --fp16


150，180，230，330，360，440



python src/export_model.py --model_name_or_path /home/wangguangle/model/baichuan2-13b-chat --template baichuan2 --finetuning_type lora --checkpoint_dir /home/wangguangle/LLaMA-Factory-0.2.0/saves/Baichuan2-13B-Chat/lora/2023-10-25-13-44/checkpoint-120 --export_dir /home/wangguangle/model/baichuan2-13b-Chat-2023-10-25-120 --fp16

120，380，450，690，790


medusa训练

CUDA_VISIBLE_DEVICES=0 python medusa/train/train.py --model_name_or_path /home/wangguangle/model/baichuan2-13b-chat     --data_path /home/wangguangle/Medusa-0.1/data/ShareGPT_V4.3_unfiltered_cleaned_split.json     --bf16 True     --output_dir test     --num_train_epochs 1     --per_device_train_batch_size 8     --per_device_eval_batch_size 8     --gradient_accumulation_steps 4     --evaluation_strategy "no"     --save_strategy "no"     --learning_rate 1e-3     --weight_decay 0.0     --warmup_ratio 0.1     --lr_scheduler_type "cosine"     --logging_steps 1     --tf32 True     --model_max_length 2048     --lazy_preprocess True     --medusa_num_heads 3     --medusa_num_layers 1









